
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>博客详情 - 分布式梯度下降算法：从同步到异步的优化策略</title>
    <link rel="stylesheet" href="../assets/detail-styles.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <!-- MathJax配置必须在脚本之前 -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body class="blog-theme">
    <!-- 导航栏 -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="../home/index.html" class="nav-brand">
                <i class="fas fa-arrow-left"></i>
                <span>返回主页</span>
            </a>
            <div class="nav-type">
                <i class="fas fa-blog"></i>
                <span>博客文章</span>
            </div>
        </div>
    </nav>

    <div class="detail-container">
        <!-- 文章标题区域 -->
        <div class="detail-header">
            <div class="header-content">
                <div class="project-badge">
                    <i class="fas fa-graduation-cap"></i>
                    <span>学术</span>
                </div>
                <h1 class="detail-title">分布式梯度下降算法：从同步到异步的优化策略</h1>
                <p class="detail-subtitle">系统性分析分布式梯度下降算法的理论基础、实现方式和收敛性质，探讨大规模机器学习中的并行优化策略。</p>
                <div class="detail-stats">
                    <div class="stat-item">
                        <i class="fas fa-eye"></i>
                        <span></span>
                    </div>
                    <div class="stat-item">
                        <i class="fas fa-heart"></i>
                        <span></span>
                    </div>
                    <div class="stat-item">
                        <i class="fas fa-calendar"></i>
                        <span>2025.09.02</span>
                    </div>
                </div>
            </div>
        </div>

        <!-- 主要内容区域 -->
        <div class="detail-content">
            <div class="content-wrapper">
                <!-- 文章内容 -->
                <div class="main-content">
                    <section class="content-section">
                        <h2>问题建模</h2>
                        <p>考虑大规模优化问题，其中数据分布在多个计算节点上：</p>
                        
                        <div class="math-block">
                            $$\min_{w \in \mathbb{R}^d} F(w) = \frac{1}{n} \sum_{i=1}^n f_i(w)$$
                        </div>
                        
                        <p>在分布式设置中，数据被分割为m个子集，每个工作节点i持有数据子集 $\mathcal{D}_i$，对应的本地目标函数为：</p>
                        
                        <div class="math-block">
                            $$F_i(w) = \frac{1}{|\mathcal{D}_i|} \sum_{j \in \mathcal{D}_i} f_j(w)$$
                        </div>

                        <h3>挑战与动机</h3>
                        <ul>
                            <li><strong>计算瓶颈</strong>：单机无法处理大规模数据</li>
                            <li><strong>内存限制</strong>：超大模型无法装入单机内存</li>
                            <li><strong>通信开销</strong>：网络延迟和带宽限制</li>
                            <li><strong>容错需求</strong>：分布式环境的可靠性挑战</li>
                        </ul>

                        <h3>设计目标</h3>
                        <ul>
                            <li><strong>线性加速比</strong>：理想情况下时间复杂度降低m倍</li>
                            <li><strong>通信效率</strong>：最小化网络传输开销</li>
                            <li><strong>收敛保证</strong>：维持算法的收敛性质</li>
                            <li><strong>容错性</strong>：应对节点故障和网络问题</li>
                        </ul>
                    </section>

                    <section class="content-section">
                        <h2>同步分布式SGD</h2>
                        
                        <h3>算法框架</h3>
                        <div class="algorithm-box">
                            <h4>算法1: 同步分布式SGD (Sync-SGD)</h4>
                            <ol>
                                <li><strong>初始化</strong>: 所有节点共享初始参数 $w_0$</li>
                                <li><strong>For</strong> t = 0, 1, 2, ... <strong>do</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>并行计算</strong>: 每个节点i计算本地梯度
                                    $$g_i^{(t)} = \nabla F_i(w^{(t)})$$
                                </li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>通信聚合</strong>: 计算全局梯度
                                    $$g^{(t)} = \frac{1}{m} \sum_{i=1}^m g_i^{(t)}$$
                                </li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>参数更新</strong>:
                                    $$w^{(t+1)} = w^{(t)} - \alpha g^{(t)}$$
                                </li>
                                <li><strong>End For</strong></li>
                            </ol>
                        </div>

                        <h3>收敛性分析</h3>
                        <div class="theorem-box">
                            <h4>定理1 (Sync-SGD收敛率)</h4>
                            <p>假设损失函数满足L-smooth和μ-strongly convex条件，则同步SGD具有线性收敛率：</p>
                            $$\mathbb{E}[F(w^{(t)}) - F(w^*)] \leq (1 - \alpha \mu)^t [F(w^{(0)}) - F(w^*)]$$
                            <p>其中学习率需满足 $\alpha \leq \frac{1}{L}$。</p>
                        </div>

                        <h3>实现细节</h3>
                        <pre><code>import torch
import torch.distributed as dist
import torch.nn as nn

class SyncDistributedSGD:
    def __init__(self, model, learning_rate=0.01):
        self.model = model
        self.lr = learning_rate
        self.world_size = dist.get_world_size()
        
    def step(self, loss):
        # 反向传播计算本地梯度
        loss.backward()
        
        # 同步所有进程的梯度
        for param in self.model.parameters():
            if param.grad is not None:
                # AllReduce操作：求和后平均
                dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
                param.grad.data /= self.world_size
                
                # 参数更新
                param.data -= self.lr * param.grad.data
                
        # 清除梯度
        self.model.zero_grad()
        
    def train_epoch(self, dataloader, criterion):
        self.model.train()
        total_loss = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            output = self.model(data)
            loss = criterion(output, target)
            
            self.step(loss)
            total_loss += loss.item()
            
            # 同步损失值用于监控
            dist.all_reduce(loss.data, op=dist.ReduceOp.SUM)
            loss.data /= self.world_size
            
        return total_loss / len(dataloader)</code></pre>

                        <h3>优势与局限</h3>
                        <div class="comparison">
                            <div class="comparison-item">
                                <h4>优势</h4>
                                <ul>
                                    <li>✅ 等价于单机SGD</li>
                                    <li>✅ 收敛性保证</li>
                                    <li>✅ 实现简单</li>
                                </ul>
                            </div>
                            <div class="comparison-item">
                                <h4>局限</h4>
                                <ul>
                                    <li>❌ 受最慢节点限制</li>
                                    <li>❌ 通信开销大</li>
                                    <li>❌ 不适合异构环境</li>
                                </ul>
                            </div>
                        </div>
                    </section>

                    <section class="content-section">
                        <h2>异步分布式SGD</h2>
                        
                        <h3>算法框架</h3>
                        <p>异步SGD中，每个节点独立进行参数更新，无需等待其他节点：</p>
                        
                        <div class="algorithm-box">
                            <h4>算法2: 异步分布式SGD (Async-SGD)</h4>
                            <ol>
                                <li><strong>初始化</strong>: 参数服务器存储全局参数 $w_0$</li>
                                <li><strong>For each worker i in parallel do</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>While</strong> 未收敛 <strong>do</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>拉取参数</strong>: $\tilde{w}_i \leftarrow$ pull from parameter server</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>计算梯度</strong>: $g_i = \nabla F_i(\tilde{w}_i)$</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>推送更新</strong>: push $g_i$ to parameter server</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>End While</strong></li>
                                <li><strong>Parameter Server更新</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>When</strong> 收到梯度 $g_i$ <strong>do</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w \leftarrow w - \alpha g_i$</li>
                            </ol>
                        </div>

                        <h3>延迟（Staleness）分析</h3>
                        <p>异步SGD的关键问题是梯度延迟。设 $\tau_i^{(t)}$ 表示第i个工作节点在第t次更新时的延迟，则实际使用的参数为 $w^{(t-\tau_i^{(t)})}$。</p>
                        
                        <div class="theorem-box">
                            <h4>定理2 (有界延迟下的收敛性)</h4>
                            <p>假设存在常数 $\tau_{max}$ 使得 $\tau_i^{(t)} \leq \tau_{max}$，且学习率满足 $\alpha \leq \frac{1}{L(1+\tau_{max})}$，则异步SGD收敛到最优解的邻域：</p>
                            $$\mathbb{E}[F(w^{(t)}) - F(w^*)] \leq \epsilon + O(\alpha \tau_{max}^2)$$
                        </div>

                        <h3>参数服务器实现</h3>
                        <pre><code>class ParameterServer:
    def __init__(self, model_params):
        self.params = {name: param.clone() for name, param in model_params.items()}
        self.lock = threading.Lock()
        self.version = 0
        
    def pull(self, worker_id):
        """工作节点拉取参数"""
        with self.lock:
            return {name: param.clone() for name, param in self.params.items()}, self.version
    
    def push(self, worker_id, gradients, learning_rate):
        """工作节点推送梯度"""
        with self.lock:
            for name, grad in gradients.items():
                if name in self.params:
                    self.params[name] -= learning_rate * grad
            self.version += 1
    
    def get_staleness(self, worker_version):
        """计算延迟程度"""
        with self.lock:
            return self.version - worker_version

class AsyncWorker:
    def __init__(self, worker_id, model, param_server):
        self.worker_id = worker_id
        self.model = model
        self.param_server = param_server
        self.local_version = 0
        
    def train_step(self, data, target, criterion, lr):
        # 拉取最新参数
        params, server_version = self.param_server.pull(self.worker_id)
        
        # 更新本地模型参数
        for name, param in self.model.named_parameters():
            if name in params:
                param.data.copy_(params[name])
        
        # 前向传播和反向传播
        self.model.zero_grad()
        output = self.model(data)
        loss = criterion(output, target)
        loss.backward()
        
        # 收集梯度
        gradients = {}
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                gradients[name] = param.grad.data.clone()
        
        # 推送梯度到参数服务器
        self.param_server.push(self.worker_id, gradients, lr)
        
        return loss.item()</code></pre>
                    </section>

                    <section class="content-section">
                        <h2>半同步方法</h2>
                        
                        <h3>Bounded Staleness</h3>
                        <p>在纯同步和异步之间寻找平衡，限制最大延迟：</p>
                        
                        <div class="algorithm-box">
                            <h4>算法3: 有界延迟SGD</h4>
                            <ol>
                                <li><strong>参数</strong>: 最大延迟阈值 $s$</li>
                                <li><strong>For each worker i do</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;计算本地梯度 $g_i$</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>If</strong> 当前延迟 $< s$ <strong>then</strong></li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;立即发送梯度</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>Else</strong></li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;等待直到延迟 $\leq s$</li>
                            </ol>
                        </div>

                        <h3>Local SGD</h3>
                        <p>减少通信频率的另一种方法是进行多轮本地更新：</p>
                        
                        <div class="algorithm-box">
                            <h4>算法4: Local SGD</h4>
                            <ol>
                                <li><strong>For</strong> round r = 1, 2, ... <strong>do</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>For each worker i in parallel do</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>For</strong> k = 1, ..., K <strong>do</strong>:</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w_i^{(k)} = w_i^{(k-1)} - \alpha \nabla F_i(w_i^{(k-1)})$</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>End For</strong></li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>End For</strong></li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>聚合</strong>: $w^{(r)} = \frac{1}{m} \sum_{i=1}^m w_i^{(K)}$</li>
                                <li>&nbsp;&nbsp;&nbsp;&nbsp;<strong>广播</strong>: $w_i^{(0)} = w^{(r)}$ for all i</li>
                                <li><strong>End For</strong></li>
                            </ol>
                        </div>

                        <div class="theorem-box">
                            <h4>定理3 (Local SGD收敛率)</h4>
                            <p>当数据是IID分布时，Local SGD的收敛率为：</p>
                            $$\mathbb{E}[F(\bar{w}^{(T)}) - F(w^*)] \leq O\left(\frac{1}{T} + \frac{K\alpha^2\sigma^2}{m}\right)$$
                            <p>其中K是本地更新次数，m是工作节点数。</p>
                        </div>
                    </section>

                    <section class="content-section">
                        <h2>通信优化技术</h2>
                        
                        <h3>All-Reduce优化</h3>
                        <p>传统的Parameter Server架构可能成为瓶颈，All-Reduce提供了更好的扩展性：</p>
                        
                        <ul>
                            <li><strong>Ring All-Reduce</strong>: 通信复杂度 $O(m)$，带宽利用率高</li>
                            <li><strong>Tree All-Reduce</strong>: 通信复杂度 $O(\log m)$，适合小数据</li>
                            <li><strong>Butterfly All-Reduce</strong>: 结合两者优势的混合方法</li>
                        </ul>

                        <h3>梯度压缩</h3>
                        <p>结合前文讨论的稀疏化技术：</p>
                        
                        <pre><code>class CompressedSGD:
    def __init__(self, model, compression_ratio=0.01, error_feedback=True):
        self.model = model
        self.compression_ratio = compression_ratio
        self.error_feedback = error_feedback
        
        if error_feedback:
            self.error_buffer = {}
            for name, param in model.named_parameters():
                self.error_buffer[name] = torch.zeros_like(param)
    
    def compress_gradient(self, grad, name):
        if self.error_feedback:
            # 误差反馈
            grad = grad + self.error_buffer[name]
        
        # Top-k压缩
        k = max(1, int(grad.numel() * self.compression_ratio))
        _, indices = torch.topk(torch.abs(grad.flatten()), k)
        
        compressed_grad = torch.zeros_like(grad)
        compressed_grad.flatten()[indices] = grad.flatten()[indices]
        
        if self.error_feedback:
            # 更新误差缓冲区
            self.error_buffer[name] = grad - compressed_grad
        
        return compressed_grad, indices
    
    def step(self, loss):
        loss.backward()
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                # 压缩梯度
                compressed_grad, _ = self.compress_gradient(param.grad.data, name)
                
                # 全局聚合（这里简化为直接使用）
                param.data -= self.lr * compressed_grad
        
        self.model.zero_grad()</code></pre>
                    </section>

                    <section class="content-section">
                        <h2>容错机制</h2>
                        
                        <h3>检查点机制</h3>
                        <p>定期保存模型状态以应对节点故障：</p>
                        
                        <pre><code>class FaultTolerantTrainer:
                    def __init__(self, model, checkpoint_dir, save_freq=100):
        self.model = model
        self.checkpoint_dir = checkpoint_dir
        self.save_freq = save_freq
        self.step_count = 0
        
    def save_checkpoint(self, epoch, loss):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'loss': loss,
            'step_count': self.step_count
        }
        
        checkpoint_path = os.path.join(
            self.checkpoint_dir, 
            f'checkpoint_epoch_{epoch}.pth'
        )
        torch.save(checkpoint, checkpoint_path)
        
    def load_checkpoint(self, checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        return checkpoint['epoch'], checkpoint['loss']
    
    def train_step_with_fault_tolerance(self, data, target, criterion, optimizer):
        try:
            # 正常训练步骤
            output = self.model(data)
            loss = criterion(output, target)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            self.step_count += 1
            
            # 定期保存检查点
            if self.step_count % self.save_freq == 0:
                self.save_checkpoint(self.step_count // self.save_freq, loss.item())
                
            return loss.item()
            
        except Exception as e:
            # 发生错误时加载最新检查点
            logger.error(f"Training error: {e}")
            self.recover_from_failure()
            raise e
    
    def recover_from_failure(self):
        # 寻找最新的检查点
        checkpoint_files = glob.glob(os.path.join(self.checkpoint_dir, "checkpoint_*.pth"))
        if checkpoint_files:
            latest_checkpoint = max(checkpoint_files, key=os.path.getctime)
            epoch, loss = self.load_checkpoint(latest_checkpoint)
            logger.info(f"Recovered from checkpoint: epoch {epoch}, loss {loss}")
        else:
            logger.warning("No checkpoint found, starting from scratch")</code></pre>

                        <h3>动态节点管理</h3>
                        <p>处理节点的加入和离开：</p>
                        
                        <ul>
                            <li><strong>节点故障检测</strong>: 心跳机制和超时检测</li>
                            <li><strong>参数重分配</strong>: 将失效节点的任务转移</li>
                            <li><strong>状态同步</strong>: 新节点加入时的参数同步</li>
                        </ul>
                    </section>

                    <section class="content-section">
                        <h2>性能评估</h2>
                        
                        <h3>实验设置</h3>
                        <table>
                            <thead>
                                <tr>
                                    <th>数据集</th>
                                    <th>模型</th>
                                    <th>节点数</th>
                                    <th>批量大小</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>CIFAR-10</td>
                                    <td>ResNet-18</td>
                                    <td>8</td>
                                    <td>128</td>
                                </tr>
                                <tr>
                                    <td>ImageNet</td>
                                    <td>ResNet-50</td>
                                    <td>16</td>
                                    <td>256</td>
                                </tr>
                                <tr>
                                    <td>GLUE</td>
                                    <td>BERT-base</td>
                                    <td>32</td>
                                    <td>32</td>
                                </tr>
                            </tbody>
                        </table>

                        <h3>收敛速度比较</h3>
                        <div class="performance-metrics">
                            <div class="metric">
                                <h4>CIFAR-10 (8节点)</h4>
                                <ul>
                                    <li>Sync-SGD: 94.2% (150 epochs)</li>
                                    <li>Async-SGD: 93.8% (200 epochs)</li>
                                    <li>Local-SGD: 94.0% (120 epochs)</li>
                                </ul>
                            </div>
                            <div class="metric">
                                <h4>通信开销</h4>
                                <ul>
                                    <li>Sync-SGD: 100% baseline</li>
                                    <li>Async-SGD: 80% (异步性)</li>
                                    <li>Local-SGD: 20% (K=5)</li>
                                </ul>
                            </div>
                        </div>

                        <h3>扩展性分析</h3>
                        <p>理想情况下，m个节点应该带来m倍的加速比。实际性能受到以下因素影响：</p>
                        
                        <ul>
                            <li><strong>通信开销</strong>: 随节点数增加而增长</li>
                            <li><strong>同步等待</strong>: 受最慢节点影响</li>
                            <li><strong>负载均衡</strong>: 数据分布和计算能力差异</li>
                        </ul>
                    </section>

                    <section class="content-section">
                        <h2>最新进展</h2>
                        
                        <h3>联邦学习</h3>
                        <p>在数据隐私要求下的分布式学习：</p>
                        
                        <ul>
                            <li><strong>FedAvg</strong>: 联邦平均算法</li>
                            <li><strong>差分隐私</strong>: 添加噪声保护隐私</li>
                            <li><strong>安全聚合</strong>: 加密的梯度聚合</li>
                        </ul>

                        <h3>自适应方法</h3>
                        <p>结合Adam等自适应优化器的分布式版本：</p>
                        
                        <div class="math-block">
                            $$m^{(t)} = \beta_1 m^{(t-1)} + (1-\beta_1) g^{(t)}$$
                            $$v^{(t)} = \beta_2 v^{(t-1)} + (1-\beta_2) (g^{(t)})^2$$
                            $$w^{(t+1)} = w^{(t)} - \alpha \frac{m^{(t)}}{\sqrt{v^{(t)}} + \epsilon}$$
                        </div>

                        <h3>模型并行</h3>
                        <p>超大模型的分布式训练策略：</p>
                        
                        <ul>
                            <li><strong>流水线并行</strong>: 将模型分层分布</li>
                            <li><strong>张量并行</strong>: 矩阵运算的分布式计算</li>
                            <li><strong>数据并行</strong>: 传统的批量数据分割</li>
                        </ul>
                    </section>

                    <section class="content-section">
                        <h2>总结</h2>
                        
                        <h3>方法选择指南</h3>
                        <ul>
                            <li><strong>同步SGD</strong>: 网络条件好，节点性能均匀</li>
                            <li><strong>异步SGD</strong>: 异构环境，容忍一定精度损失</li>
                            <li><strong>Local SGD</strong>: 带宽受限，追求通信效率</li>
                        </ul>

                        <h3>未来方向</h3>
                        <ul>
                            <li><strong>硬件协同</strong>: 与新型硬件的深度集成</li>
                            <li><strong>自动调优</strong>: 基于系统状态的参数自适应</li>
                            <li><strong>绿色计算</strong>: 考虑能耗的优化策略</li>
                            <li><strong>跨域协作</strong>: 多机构间的安全协作学习</li>
                        </ul>
                        
                        <p>分布式梯度下降算法为大规模机器学习提供了强大的工具，但在实际应用中需要根据具体场景选择合适的策略。随着硬件技术和算法理论的不断发展，我们期待看到更加高效和智能的分布式优化方法。</p>
                    </section>
                </div>

                <!-- 侧边栏 -->
                <div class="sidebar">
                    <div class="sidebar-section">
                        <h3>目录</h3>
                        <ul class="toc">
                            <li><a href="#modeling">问题建模</a></li>
                            <li><a href="#sync">同步SGD</a></li>
                            <li><a href="#async">异步SGD</a></li>
                            <li><a href="#semi-sync">半同步方法</a></li>
                            <li><a href="#communication">通信优化</a></li>
                            <li><a href="#fault-tolerance">容错机制</a></li>
                            <li><a href="#evaluation">性能评估</a></li>
                            <li><a href="#advances">最新进展</a></li>
                        </ul>
                    </div>

                    <div class="sidebar-section">
                        <h3>相关技术</h3>
                        <div class="tags">
                            <span class="tag">分布式优化</span>
                            <span class="tag">同步SGD</span>
                            <span class="tag">异步SGD</span>
                            <span class="tag">参数服务器</span>
                            <span class="tag">All-Reduce</span>
                        </div>
                    </div>

                    <div class="sidebar-section">
                        <h3>其他博客</h3>
                        <ul class="related-posts">
                            <li><a href="blog-sparsification.html">梯度稀疏化方法</a></li>
                            <li><a href="blog-gradient-coding.html">梯度编码技术</a></li>
                            <li><a href="blog-fastest-k.html">Fastest-k SGD算法</a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        // MathJax已在head部分配置
    </script>
    <script src="../assets/detail-script.js"></script>
</body>
</html>
